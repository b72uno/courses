{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rover Lab Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# An option to plot to an interactive window\n",
    "#%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV for perspective transform\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc # For saving images as needed\n",
    "import glob # For reading in a list of images from a folder\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '../course_test_dataset/IMG/*'\n",
    "path = '../test_dataset/IMG/*'\n",
    "img_list = glob.glob(path)\n",
    "\n",
    "# Grab a random image and display it\n",
    "idx = np.random.randint(0, len(img_list)-1)\n",
    "image = mpimg.imread(img_list[idx])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Data\n",
    "\n",
    "Read in and display example grid and rock sample calibration images. Use the grid for perspective transform and the rock image for creating a new color selection that identifies these samples of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the simulator you can toggle on a grid on the ground for calibration\n",
    "# You can also toggle on the rock samples \n",
    "example_grid = '../calibration_images/example_grid1.jpg'\n",
    "example_rock = '../calibration_images/example_rock1.jpg'\n",
    "grid_img = mpimg.imread(example_grid)\n",
    "rock_img = mpimg.imread(example_rock)\n",
    "\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(grid_img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(rock_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Transform\n",
    "\n",
    "Define the perspective transform function from the lesson and test it on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform a perspective transform\n",
    "def perspect_transform(img, src, dest):\n",
    "    M = cv2.getPerspectiveTransform(src, dest)\n",
    "    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0])) # keep the same size as input\n",
    "    return warped\n",
    "\n",
    "# Define a calibration box in source (actual) and destination (desired) coordinates\n",
    "# The source and destination points are defined to warp the image to a grid\n",
    "# where each 10x10 pixel square represents 1 square meter\n",
    "# The destination box will be 2 * dst_size on each side\n",
    "dst_size = 5\n",
    "\n",
    "# Set a bottom offset - bottom offset is in front of the rover a bit\n",
    "bottom_offset = 5\n",
    "\n",
    "source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "destination = np.float32([[image.shape[1]/2 - dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - 2*dst_size - bottom_offset], \n",
    "                  [image.shape[1]/2 - dst_size, image.shape[0] - 2*dst_size - bottom_offset],\n",
    "                  ])\n",
    "\n",
    "warped = perspect_transform(grid_img, source, destination)\n",
    "\n",
    "plt.imshow(warped)\n",
    "scipy.misc.imsave('../output/warped.jpg', warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_image(img):\n",
    "    # Mask the field of view partially\n",
    "    # used later to experiment with fidelity\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    image = np.zeros_like(imgGray)\n",
    "    radius = np.int32(img.shape[0] / 2)\n",
    "    axes = (radius,radius)\n",
    "    angle = 0;\n",
    "    startAngle = 0;\n",
    "    endAngle = -180;\n",
    "    center_X = np.int32(img.shape[1] / 2)\n",
    "    center_Y = np.int32(img.shape[0])\n",
    "    center = (center_X, center_Y)\n",
    "    color = 255\n",
    "    mask = cv2.ellipse(image, center, axes, angle, startAngle, endAngle, color, -1)\n",
    "    plt.imshow(mask)\n",
    "    masked = cv2.bitwise_and(img, img, mask=mask)\n",
    "    \n",
    "    return masked\n",
    "\n",
    "warped2 = mask_image(warped)\n",
    "plt.imshow(warped2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Thresholding\n",
    "Define the color thresholding function from the lesson and apply it to the warped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ultimately, you want your map to not just include navigable terrain but \n",
    "# also obstacles and the positions of the rock samples you're searching for. \n",
    "# Modify this function or write a new function that returns the pixel locations of \n",
    "# obstacles (areas below the threshold) and rock samples (yellow rocks in calibration images), \n",
    "# such that you can map these areas into world coordinates as well.\n",
    "import operator\n",
    "\n",
    "# Identify pixels above the threshold\n",
    "# Above 160 works well for ground pixels only\n",
    "def color_thresh(img, rgb_thresh=(160, 160, 160), cmp_op=operator.gt):\n",
    "    # Single channel of img shaped zeros\n",
    "    color_select = np.zeros_like(img[:,:,0])\n",
    "    \n",
    "    # separate channels\n",
    "    img_r = img[:,:,0]\n",
    "    img_g = img[:,:,1]\n",
    "    img_b = img[:,:,2]\n",
    "    \n",
    "    r, g, b = rgb_thresh\n",
    "\n",
    "    # Compare each pix to thresh\n",
    "    thresh = (cmp_op(img_r, r) \\\n",
    "              & cmp_op(img_g, g) \\\n",
    "              & cmp_op(img_b, b))\n",
    "        \n",
    "    color_select[thresh] = 1\n",
    "    \n",
    "    return color_select\n",
    "\n",
    "threshed = color_thresh(warped)\n",
    "plt.imshow(threshed, cmap='gray')\n",
    "#scipy.misc.imsave('../output/warped_threshed.jpg', threshed*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_thresh_terrain(img):\n",
    "    return color_thresh(img, (160,160,160))\n",
    "\n",
    "def color_thresh_obstacles(img):\n",
    "    return color_thresh_terrain(img) ^ 1\n",
    "\n",
    "def color_thresh_samples(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([30, 255, 255])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "img = mpimg.imread('../calibration_images/example_rock3.jpg')\n",
    "#img = perspect_transform(img, source, destination)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.subplot(221)\n",
    "plt.imshow(img)\n",
    "plt.subplot(222)\n",
    "plt.imshow(color_thresh_terrain(img), cmap='gray')\n",
    "plt.subplot(223)\n",
    "plt.imshow(color_thresh_obstacles(img), cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.imshow(color_thresh_samples(img), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Transformations\n",
    "Define the functions used to do the coordinate transforms and apply them to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rover_coords(binary_img):\n",
    "    # Identify nonzero pixels\n",
    "    ypos, xpos = binary_img.nonzero()\n",
    "    \n",
    "    # Calculate pixel positions with reference to the rover position\n",
    "    # being at the center bottom of the image\n",
    "    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n",
    "    y_pixel = -(xpos - binary_img.shape[1]/2).astype(np.float)\n",
    "    \n",
    "    return x_pixel, y_pixel\n",
    "\n",
    "def to_polar_coords(x_pixel, y_pixel):\n",
    "    # Convert to radial coords\n",
    "    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n",
    "    angles = np.arctan2(y_pixel, x_pixel)\n",
    "    \n",
    "    return dist, angles\n",
    "\n",
    "def rotate_pix(xpix, ypix, yaw):\n",
    "    # Apply rotation to pixel positions, input yaw angle in degrees\n",
    "    yaw_rad = yaw * np.pi / 180\n",
    "    x_rotated = xpix * np.cos(yaw_rad) - ypix * np.sin(yaw_rad)\n",
    "    y_rotated = xpix * np.sin(yaw_rad) + ypix * np.cos(yaw_rad)\n",
    "    \n",
    "    return x_rotated, y_rotated\n",
    "\n",
    "def translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale):\n",
    "    # Perform translation and convert to integer since pixel values cant be float\n",
    "    xpix_translated = np.int_(xpos + (xpix_rot / scale))\n",
    "    ypix_translated = np.int_(ypos + (ypix_rot / scale))\n",
    "    \n",
    "    return xpix_translated, ypix_translated\n",
    "\n",
    "def pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n",
    "    # Define a function to apply rotation and translation (and clipping)\n",
    "    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n",
    "    \n",
    "    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n",
    "    \n",
    "    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n",
    "    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n",
    "  \n",
    "    return x_pix_world, y_pix_world\n",
    "\n",
    "# Grab another random image\n",
    "idx = np.random.randint(0, len(img_list)-1)\n",
    "image = mpimg.imread(img_list[idx])\n",
    "warped = perspect_transform(image, source, destination)\n",
    "threshed = color_thresh(warped)\n",
    "\n",
    "# Calculate pixel values in rover-centric coords and distance/angle to all pixels\n",
    "xpix, ypix = rover_coords(threshed)\n",
    "dist, angles = to_polar_coords(xpix, ypix)\n",
    "mean_dir = np.mean(angles)\n",
    "\n",
    "# Do some plotting\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "plt.subplot(221)\n",
    "plt.imshow(image)\n",
    "plt.subplot(222)\n",
    "plt.imshow(warped)\n",
    "plt.subplot(223)\n",
    "plt.imshow(threshed, cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.plot(xpix, ypix, '.')\n",
    "plt.ylim(-160, 160)\n",
    "plt.xlim(0, 160)\n",
    "arrow_length = 100\n",
    "x_arrow = arrow_length * np.cos(mean_dir)\n",
    "y_arrow = arrow_length * np.sin(mean_dir)\n",
    "plt.arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in saved data and ground truth map of the world\n",
    "\n",
    "The next cell is all setup to read your saved data into a pandas dataframe. Here you'll also read in a \"ground truth\" map of the world, where white pixels (pixel value = 1) represent navigable terrain.\n",
    "\n",
    "After that, we'll define a class to store telemetry data and pathnames to images. When you instantiate this class (data = Databucket()) you'll have a global variable called data that you can refer to for telemetry and map data within the process_image() function in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../test_dataset/robot_log.csv', sep=';', decimal='.')\n",
    "#print(list(df.columns.values))\n",
    "csv_img_list = df[\"Path\"].tolist() \n",
    "\n",
    "# Read in ground truth map and create a 3 channel image with it\n",
    "ground_truth = mpimg.imread('../calibration_images/map_bw.png')\n",
    "ground_truth_3d = np.dstack((ground_truth*0, ground_truth*255, ground_truth*0)).astype(np.float)\n",
    "\n",
    "# Create a class to be the data container\n",
    "class Databucket():\n",
    "    def __init__(self):\n",
    "        self.images = csv_img_list\n",
    "        self.xpos = df[\"X_Position\"].values\n",
    "        self.ypos = df[\"Y_Position\"].values\n",
    "        self.yaw = df[\"Yaw\"].values\n",
    "        self.count = -1 # A running index, -1 is a hack for moviepy, an extra iter\n",
    "        self.worldmap = np.zeros((200, 200, 3)).astype(np.float)\n",
    "        self.ground_truth = ground_truth_3d # Ground truth worldmap\n",
    "        \n",
    "# Instantiate a Databucket... will be a global variable/object\n",
    "# that you can refer to in the process_image() function below\n",
    "\n",
    "data = Databucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to process stored images\n",
    "\n",
    "Modify the process_image() function below by adding in the perception step processes (functions defined above) to perform image analysis and mapping. The following cell is all set up to use this process_image() function in conjunction with the moviepy video processing package to create a video from the images you saved taking data in the simulator.\n",
    "\n",
    "In short, you will be passing individual images into process_image() and building up an image called output_image that will be stored as one frame of video. You can make a mosaic of the various steps of your analysis process and add text as you like (example provided below).\n",
    "\n",
    "To start with, you can simply run the next three cells to see what happens, but then go ahead and modify them such that the output video demonstrates your mapping process. Feel free to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    # Pass stored images to reading \n",
    "    # rover position and yaw angle from csv file\n",
    "    \n",
    "    # TODO:\n",
    "    # 1) Define source and destination points for perspective transform\n",
    "    \n",
    "    # Define a calibration box in source (actual) and destination (desired) coordinates\n",
    "    # The source and destination points are defined to warp the image to a grid\n",
    "    # where each 10x10 pixel square represents 1 square meter\n",
    "    # The destination box will be 2 * dst_size on each side\n",
    "    dst_size = 5\n",
    "\n",
    "    # Set a bottom offset - bottom offset is in front of the rover a bit\n",
    "    bottom_offset = 5\n",
    "    \n",
    "    image = img\n",
    "    source = np.float32([[14, 140], [301 ,140], [200, 96], [118, 96]])\n",
    "    destination = np.float32([[image.shape[1]/2 - dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - bottom_offset],\n",
    "                  [image.shape[1]/2 + dst_size, image.shape[0] - 2*dst_size - bottom_offset], \n",
    "                  [image.shape[1]/2 - dst_size, image.shape[0] - 2*dst_size - bottom_offset],\n",
    "                  ])\n",
    "    \n",
    "    # 2) Apply color threshold to identify navigable terrain/obstacles/rock samples\n",
    "    # NOTE: Doing threshold before transform\n",
    "    terrain = color_thresh_terrain(img)\n",
    "    obstacles = color_thresh_obstacles(img)\n",
    "    samples = color_thresh_samples(img)\n",
    "    \n",
    "    # 3) Apply perspective transform\n",
    "    warped_terrain = perspect_transform(terrain, source, destination)\n",
    "    warped_obstacles = perspect_transform(obstacles, source, destination)\n",
    "    warped_samples = perspect_transform(samples, source, destination)\n",
    "    \n",
    "    # 4) Convert thresholded image pixel values to rover-centric coords\n",
    "    rover_centric_terrain = rover_coords(warped_terrain)\n",
    "    rover_centric_obstacles = rover_coords(warped_obstacles)\n",
    "    rover_centric_samples = rover_coords(warped_samples)\n",
    "    \n",
    "    # 5) Convert rover-centric pixel values to world coords\n",
    "    scale = 10\n",
    "    #  pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale)\n",
    "    def px2world(rov_coords): \n",
    "        return pix_to_world(rov_coords[0], rov_coords[1], data.xpos[data.count], \n",
    "                            data.ypos[data.count], data.yaw[data.count], \n",
    "                            data.worldmap.shape[0], scale)\n",
    "    world_terrain = px2world(rover_centric_terrain)\n",
    "    world_obstacles = px2world(rover_centric_obstacles)\n",
    "    world_samples = px2world(rover_centric_samples)\n",
    "    \n",
    "    # 6) Update worldmap to be displayed on right side of screen\n",
    "    #    Example: data.worldmap[obstackle_y_world, obstacle_x_world, 0] += 1\n",
    "    #             data.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "    #             data.worldmap[navigable_y_world, navigable_x_world, 2] += 1\n",
    "    #\n",
    "    # NOTE: X and Y are reversed\n",
    "    data.worldmap[world_obstacles[1], world_obstacles[0], 0] += 1\n",
    "    data.worldmap[world_terrain[1], world_terrain[0], 2] += 10\n",
    "\n",
    "    if samples.any():\n",
    "        \n",
    "        sample_dist, sample_angles = to_polar_coords(rover_centric_samples[0],\n",
    "                                                     rover_centric_samples[1])\n",
    "        sample_idx = np.argmin(sample_dist)\n",
    "        sample_cX = world_samples[0][sample_idx]\n",
    "        sample_cY = world_samples[1][sample_idx]\n",
    "        \n",
    "        data.worldmap[np.int_(sample_cY), np.int_(sample_cX), 1] = 255\n",
    "        \n",
    "    # 7) Make a mosaic image, below is some sample code\n",
    "    \n",
    "    # First create a blank image, shape whatever\n",
    "    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n",
    "    \n",
    "    # Next, populate regions with various output\n",
    "    # e.g. putting original image in the upper left hand corner\n",
    "    output_image[0:img.shape[0], 0:img.shape[1]] = img\n",
    "    \n",
    "    # Create more images to add to the mosaic, first a warped image\n",
    "    warped = perspect_transform(img, source, destination)\n",
    "\n",
    "    # Add the warped image in the upper right\n",
    "    output_image[0:img.shape[0], img.shape[1]:] = warped\n",
    "    \n",
    "    # Add vision to lower right\n",
    "    img_x = img.shape[0]\n",
    "    img_y = img.shape[1]\n",
    "    output_image[-img_x:, img_y:, 2] = warped_terrain * 255\n",
    "    output_image[-img_x:, img_y:, 0] = warped_obstacles * 255\n",
    "    output_image[-img_x:, img_y:, 1] = warped_samples * 255\n",
    "    \n",
    "    # Overlay worldmap with ground truth map\n",
    "    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n",
    "    \n",
    "    # Flip map overlay so y-axis points upward and add to output_image\n",
    "    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n",
    "     \n",
    "    data.count += 1 # Keep track of index in the Databucket()\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a video from processed image data\n",
    "Use the moviepy library to process images and create a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Define pathname to save the output\n",
    "output = '../output/test_mapping.mp4'\n",
    "data = Databucket() # Re-initialize in case running multiple times\n",
    "clip = ImageSequenceClip(data.images, fps=60) \n",
    "# Note that output will be sped up \n",
    "# because recording rate in simulator is 25 fps\n",
    "\n",
    "new_clip = clip.fl_image(process_image) # NOTE: This function expects color images!\n",
    "%time new_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next cell shold function as an inline video player\n",
    "If this fails to render the video, try running the following cell. Or check the output folder mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../output/test_mapping.mp4'\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "video = io.open(output, 'r+b').read()\n",
    "encoded_video = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "            </vide>'''.format(encoded_video.decode('ascii')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
